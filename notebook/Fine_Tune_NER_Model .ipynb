{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load your labeled dataset\n",
    "df = pd.read_csv(r'C:\\Users\\Yibabe\\Desktop\\10academyAIMweek-5\\data\\labeled_messages.csv')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labeled_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>·ä¶·à™·åÖ·äì·àç                0          B-PRODUCT\\n·ãï·âÉ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3                    0          B-PRODUCT\\n1  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>·ã®·çí·ãõ                  0          B-PRODUCT\\n·àò·åã·åà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·ã®·àª·ãà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·à≥·àÖ·äï...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     labeled_message\n",
       "0  ·ä¶·à™·åÖ·äì·àç                0          B-PRODUCT\\n·ãï·âÉ ...\n",
       "1  3                    0          B-PRODUCT\\n1  ...\n",
       "2  ·ã®·çí·ãõ                  0          B-PRODUCT\\n·àò·åã·åà...\n",
       "3  ·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·ã®·àª·ãà...\n",
       "4  ·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·à≥·àÖ·äï..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yibabe\\Desktop\\10academyAIMweek-5\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Step 1: Load your CSV file into a pandas DataFrame\n",
    "# Make sure to specify the correct path to your CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\Yibabe\\Desktop\\10academyAIMweek-5\\data\\labeled_messages.csv', header=None, names=['token', 'start_pos', 'label'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Initialize lists to hold the sentences and their corresponding labels\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "# Temporary lists to accumulate tokens and labels for a single sentence\n",
    "current_sentence = []\n",
    "current_labels = []\n",
    "\n",
    "# Iterate through the DataFrame to build sentences and labels\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the current row contains a valid token\n",
    "    if pd.notna(row['token']) and row['token'].strip():  # Skip NaN and empty tokens\n",
    "        current_sentence.append(row['token'])\n",
    "        current_labels.append(row['label'])\n",
    "    else:\n",
    "        # If the current token is empty, it indicates the end of a sentence\n",
    "        if current_sentence:  # If there are accumulated tokens\n",
    "            sentences.append(current_sentence)\n",
    "            labels.append(current_labels)\n",
    "            current_sentence = []  # Reset for the next sentence\n",
    "            current_labels = []\n",
    "\n",
    "# Don't forget to add the last sentence if there's no empty line at the end\n",
    "if current_sentence:\n",
    "    sentences.append(current_sentence)\n",
    "    labels.append(current_labels)\n",
    "\n",
    "# Step 3: Create a Hugging Face Dataset from the lists\n",
    "dataset = Dataset.from_dict({'tokens': sentences, 'ner_tags': labels})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 1\n",
      "})\n",
      "Number of rows in the dataset: 1\n"
     ]
    }
   ],
   "source": [
    "# Optionally, display the first few entries to verify the dataset structure\n",
    "print(dataset)\n",
    "print(f\"Number of rows in the dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"xlm-roberta-base\"  # You can also use bert-tiny-amharic if available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=5)  # Adjust num_labels based on your entity types (e.g., products, prices, locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset with `trust_remote_code=True`\n",
    "dataset = load_dataset(\"conll2003\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# Check the first few rows to ensure it's loaded correctly\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yibabe\\Desktop\\10academyAIMweek-5\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the XLM-Roberta tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the entire data from the CSV file\n",
    "data_file_path = r'C:\\Users\\Yibabe\\Desktop\\10academyAIMweek-5\\data\\labeled_messages.csv' \n",
    "df = pd.read_csv(data_file_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labeled_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>·ä¶·à™·åÖ·äì·àç                0          B-PRODUCT\\n·ãï·âÉ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3                    0          B-PRODUCT\\n1  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>·ã®·çí·ãõ                  0          B-PRODUCT\\n·àò·åã·åà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·ã®·àª·ãà...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·à≥·àÖ·äï...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     labeled_message\n",
       "0  ·ä¶·à™·åÖ·äì·àç                0          B-PRODUCT\\n·ãï·âÉ ...\n",
       "1  3                    0          B-PRODUCT\\n1  ...\n",
       "2  ·ã®·çí·ãõ                  0          B-PRODUCT\\n·àò·åã·åà...\n",
       "3  ·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·ã®·àª·ãà...\n",
       "4  ·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·à≥·àÖ·äï..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming the relevant text column is named 'Labeled_Message'; adjust this if needed\n",
    "if 'labeled_message' not in df.columns:\n",
    "    raise ValueError(\"The 'Labeled_Message' column is not found in the DataFrame.\")\n",
    "\n",
    "# Redundant messages to be removed\n",
    "redundant_message = [\n",
    "    \"·ä®·çç·àà·ãç O\",\n",
    "    \"·ä´·àâ·â†·âµ O\",\n",
    "    \"·ä•·äï·àç·ä´·àà·äï O\",\n",
    "    \"·ä≠·çç·àà·àÉ·åà·à≠ O\",\n",
    "    \"·àã·àã·âΩ·àÅ O\",\n",
    "    \"·ã∞·äï·â†·äû·âª·âΩ·äï O\",\n",
    "    \"·ã®·çà·àà·åâ·âµ·äï O\",\n",
    "    \"·ãï·âÉ O\",\n",
    "    \"·â†·àò·àç·ãï·ä≠·âµ O\",\n",
    "    \"·ä•·äï·àç·ä≠·àç·ãé·â≥·àà·äï O\",\n",
    "    \"·àà·àõ·ãò·ãù O\",\n",
    "    \"@ordermertteka1 O\",\n",
    "    \"@ordermertteka2 O\",\n",
    "    \"·àà·ãà·ã≥·åÖ·ãé O\",\n",
    "    \"forward O\",\n",
    "    \"·â†·àõ·ãµ·à®·åç O\",\n",
    "    \"·ã≠·â∞·â£·â†·à©·äï O\",\n",
    "    \"0944-22-23-24 O\",\n",
    "    \"0904-94-48-48 O\",\n",
    "    \"·ä†·ãµ·à´·àª·âΩ·äï O\",\n",
    "    \"·àò·åà·äì·äõ I-LOC\",\n",
    "    \"·ãò·çç·àò·àΩ O\",\n",
    "    \"·åç·à´·äï·ãµ O\",\n",
    "    \"·àû·àç O\",\n",
    "    \"3·äõ O\",\n",
    "    \"·çé·âÖ O\",\n",
    "    \"·ä®·àä·çç·âµ O\",\n",
    "    \"·à≤·ãà·à≠·ã± O\",\n",
    "    \"·ãà·ã∞ O\",\n",
    "    \"·âÄ·äù O\",\n",
    "    \"·â≥·å•·çà·ãç O\",\n",
    "    \"·âÄ·å•·â≥ O\",\n",
    "    \"376 I-LOC\",\n",
    "    \"·â†·ä™·àµ·ãé O\",\n",
    "    \"·å•·à¨ O\",\n",
    "    \"·åà·äï·ãò·â• O\",\n",
    "    \"·ä´·àç·ã´·ãô O\",\n",
    "    \"·â†·àû·â£·ã≠·àç O\",\n",
    "    \"·àõ·àµ·â∞·àã·àà·çç O\",\n",
    "    \"·ã≠·âΩ·àã·àâ·ç¢ O\",\n",
    "    \"·ã≠·àÑ·äï·äï O\",\n",
    "    \"t.me/MerttEka O\",\n",
    "    \"·â∞·å≠·äê·ãç O\",\n",
    "    \"join O\",\n",
    "    \"·ã´·ãµ·à≠·åâ·ç£ O\",\n",
    "    \"·â§·â∞·à∞·â• O\",\n",
    "    \"·ã≠·àÅ·äë O\"\n",
    "]\n",
    "\n",
    "# Function to clean redundant messages\n",
    "def clean_redundant_messages(text):\n",
    "    # Check if the text is a string\n",
    "    if isinstance(text, str):\n",
    "        for message in set(redundant_message):\n",
    "            text = text.replace(message + \"\\n\", \"\")\n",
    "        return text.strip()  # Remove leading/trailing whitespace after replacement\n",
    "    return None  # Return None for non-string entries\n",
    "\n",
    "# Clean the redundant messages in the DataFrame\n",
    "df['cleaned_text'] = df['labeled_message'].apply(clean_redundant_messages)\n",
    "\n",
    "# Function to split tokens and labels\n",
    "def split_tokens_and_labels(text):\n",
    "    if isinstance(text, str):  # Ensure the input is a string\n",
    "        lines = text.strip().split('\\n')\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            if line.strip():  # Ensure the line is not empty\n",
    "                token, label = line.rsplit(' ', 1)\n",
    "                tokens.append(token)\n",
    "                labels.append(label)\n",
    "        return tokens, labels\n",
    "    else:\n",
    "        return [], []  # Return empty lists for non-string entries\n",
    "\n",
    "# Apply the token splitting\n",
    "df['tokens_labels'] = df['cleaned_text'].apply(split_tokens_and_labels)\n",
    "\n",
    "# Separate tokens and labels into two new columns\n",
    "df['tokens'] = df['tokens_labels'].apply(lambda x: x[0])\n",
    "df['labels'] = df['tokens_labels'].apply(lambda x: x[1])\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "# output_file_path = r'C:\\Users\\Yibabe\\Desktop\\10academyAIMweek-5\\data\\cleaned_data.csv' # Update this to your desired output path\n",
    "# df.to_csv(output_file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>·ä¶·à™·åÖ·äì·àç                0          B-PRODUCT\\n·ãï·âÉ ...</td>\n",
       "      <td>[·ä¶·à™·åÖ·äì·àç                0         , ·ãï·âÉ          ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3                    0          B-PRODUCT\\n1  ...</td>\n",
       "      <td>[3                    0         , 1           ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>·ã®·çí·ãõ                  0          B-PRODUCT\\n·àò·åã·åà...</td>\n",
       "      <td>[·ã®·çí·ãõ                  0         , ·àò·åã·åà·à™·ã´       ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·ã®·àª·ãà...</td>\n",
       "      <td>[·ã®·àÖ·çÉ·äì·âµ                0         , ·ã®·àª·ãà·à≠        ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, I-PRICE, I-PRICE, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·à≥·àÖ·äï...</td>\n",
       "      <td>[·ã®·àò·àò·åà·â¢·ã´               0         , ·à≥·àÖ·äï         ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>·ã®·àù·åç·â•                 0          B-PRODUCT\\n·àõ·â•·à∞...</td>\n",
       "      <td>[·ã®·àù·åç·â•                 0         , ·àõ·â•·à∞·ã´        ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>·ã≠·àÑ·äï·äï                 0          B-PRODUCT\\n·â∞·å≠·äê...</td>\n",
       "      <td>[·ã≠·àÑ·äï·äï                 0         , ·â∞·å≠·äê·ãç        ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>·ãò·àò·äì·ãä                 0          B-PRODUCT\\n·ã®·å´·àõ...</td>\n",
       "      <td>[·ãò·àò·äì·ãä                 0         , ·ã®·å´·àõ         ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>·ã®·àª·ã≠·â°·äì·àõ·ä™·ã´·â∂            0          B-PRODUCT\\n·àò·å†·å´...</td>\n",
       "      <td>[·ã®·àª·ã≠·â°·äì·àõ·ä™·ã´·â∂            0         , ·àò·å†·å´         ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>32                   0          B-PRODUCT\\n·ã®·âº·ãù...</td>\n",
       "      <td>[32                   0         , ·ã®·âº·ãù         ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          cleaned_text  \\\n",
       "0    ·ä¶·à™·åÖ·äì·àç                0          B-PRODUCT\\n·ãï·âÉ ...   \n",
       "1    3                    0          B-PRODUCT\\n1  ...   \n",
       "2    ·ã®·çí·ãõ                  0          B-PRODUCT\\n·àò·åã·åà...   \n",
       "3    ·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·ã®·àª·ãà...   \n",
       "4    ·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·à≥·àÖ·äï...   \n",
       "..                                                 ...   \n",
       "195  ·ã®·àù·åç·â•                 0          B-PRODUCT\\n·àõ·â•·à∞...   \n",
       "196  ·ã≠·àÑ·äï·äï                 0          B-PRODUCT\\n·â∞·å≠·äê...   \n",
       "197  ·ãò·àò·äì·ãä                 0          B-PRODUCT\\n·ã®·å´·àõ...   \n",
       "198  ·ã®·àª·ã≠·â°·äì·àõ·ä™·ã´·â∂            0          B-PRODUCT\\n·àò·å†·å´...   \n",
       "199  32                   0          B-PRODUCT\\n·ã®·âº·ãù...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0    [·ä¶·à™·åÖ·äì·àç                0         , ·ãï·âÉ          ...   \n",
       "1    [3                    0         , 1           ...   \n",
       "2    [·ã®·çí·ãõ                  0         , ·àò·åã·åà·à™·ã´       ...   \n",
       "3    [·ã®·àÖ·çÉ·äì·âµ                0         , ·ã®·àª·ãà·à≠        ...   \n",
       "4    [·ã®·àò·àò·åà·â¢·ã´               0         , ·à≥·àÖ·äï         ...   \n",
       "..                                                 ...   \n",
       "195  [·ã®·àù·åç·â•                 0         , ·àõ·â•·à∞·ã´        ...   \n",
       "196  [·ã≠·àÑ·äï·äï                 0         , ·â∞·å≠·äê·ãç        ...   \n",
       "197  [·ãò·àò·äì·ãä                 0         , ·ã®·å´·àõ         ...   \n",
       "198  [·ã®·àª·ã≠·â°·äì·àõ·ä™·ã´·â∂            0         , ·àò·å†·å´         ...   \n",
       "199  [32                   0         , ·ã®·âº·ãù         ...   \n",
       "\n",
       "                                                labels  \n",
       "0    [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "1    [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "2    [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "3    [B-PRODUCT, O, O, O, O, O, I-PRICE, I-PRICE, O...  \n",
       "4    [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, I...  \n",
       "..                                                 ...  \n",
       "195  [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "196                            [B-PRODUCT, O, O, O, O]  \n",
       "197  [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "198  [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "199  [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the resulting DataFrame (optional)\n",
    "df[['cleaned_text', 'tokens', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [·ä¶·à™·åÖ·äì·àç                0         , ·ãï·âÉ          ...\n",
       "1    [3                    0         , 1           ...\n",
       "2    [·ã®·çí·ãõ                  0         , ·àò·åã·åà·à™·ã´       ...\n",
       "3    [·ã®·àÖ·çÉ·äì·âµ                0         , ·ã®·àª·ãà·à≠        ...\n",
       "4    [·ã®·àò·àò·åà·â¢·ã´               0         , ·à≥·àÖ·äï         ...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...\n",
       "1    [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...\n",
       "2    [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...\n",
       "3    [B-PRODUCT, O, O, O, O, O, I-PRICE, I-PRICE, O...\n",
       "4    [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, I...\n",
       "Name: labels, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to split cleaned text into tokens and labels\n",
    "def split_tokens_labels(row):\n",
    "    cleaned_text = row['cleaned_text']\n",
    "    \n",
    "    # Check if cleaned_text is None or an empty string\n",
    "    if cleaned_text is None or cleaned_text.strip() == \"\":\n",
    "        return ([], []), [], []  # Return empty lists if cleaned_text is not valid\n",
    "    \n",
    "    # Split the cleaned text into lines\n",
    "    lines = cleaned_text.strip().split('\\n')\n",
    "    \n",
    "    # Initialize lists for tokens and labels\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate through each line\n",
    "    for line in lines:\n",
    "        # Split each line by space\n",
    "        parts = line.rsplit(' ', 1)  # Split only at the last space\n",
    "        if len(parts) == 2:  # Ensure there's a token and a label\n",
    "            token, label = parts\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "    \n",
    "    # Create tokens_labels format as a tuple of lists\n",
    "    tokens_labels = (tokens, labels)\n",
    "    \n",
    "    return tokens_labels, tokens, labels\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df[['tokens_labels', 'tokens', 'labels']] = df.apply(split_tokens_labels, axis=1, result_type='expand')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens_labels</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>·ä¶·à™·åÖ·äì·àç                0          B-PRODUCT\\n·ãï·âÉ ...</td>\n",
       "      <td>([·ä¶·à™·åÖ·äì·àç                0         , ·ãï·âÉ         ...</td>\n",
       "      <td>[·ä¶·à™·åÖ·äì·àç                0         , ·ãï·âÉ          ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3                    0          B-PRODUCT\\n1  ...</td>\n",
       "      <td>([3                    0         , 1          ...</td>\n",
       "      <td>[3                    0         , 1           ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>·ã®·çí·ãõ                  0          B-PRODUCT\\n·àò·åã·åà...</td>\n",
       "      <td>([·ã®·çí·ãõ                  0         , ·àò·åã·åà·à™·ã´      ...</td>\n",
       "      <td>[·ã®·çí·ãõ                  0         , ·àò·åã·åà·à™·ã´       ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·ã®·àª·ãà...</td>\n",
       "      <td>([·ã®·àÖ·çÉ·äì·âµ                0         , ·ã®·àª·ãà·à≠       ...</td>\n",
       "      <td>[·ã®·àÖ·çÉ·äì·âµ                0         , ·ã®·àª·ãà·à≠        ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, I-PRICE, I-PRICE, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·à≥·àÖ·äï...</td>\n",
       "      <td>([·ã®·àò·àò·åà·â¢·ã´               0         , ·à≥·àÖ·äï        ...</td>\n",
       "      <td>[·ã®·àò·àò·åà·â¢·ã´               0         , ·à≥·àÖ·äï         ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0  ·ä¶·à™·åÖ·äì·àç                0          B-PRODUCT\\n·ãï·âÉ ...   \n",
       "1  3                    0          B-PRODUCT\\n1  ...   \n",
       "2  ·ã®·çí·ãõ                  0          B-PRODUCT\\n·àò·åã·åà...   \n",
       "3  ·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·ã®·àª·ãà...   \n",
       "4  ·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·à≥·àÖ·äï...   \n",
       "\n",
       "                                       tokens_labels  \\\n",
       "0  ([·ä¶·à™·åÖ·äì·àç                0         , ·ãï·âÉ         ...   \n",
       "1  ([3                    0         , 1          ...   \n",
       "2  ([·ã®·çí·ãõ                  0         , ·àò·åã·åà·à™·ã´      ...   \n",
       "3  ([·ã®·àÖ·çÉ·äì·âµ                0         , ·ã®·àª·ãà·à≠       ...   \n",
       "4  ([·ã®·àò·àò·åà·â¢·ã´               0         , ·à≥·àÖ·äï        ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [·ä¶·à™·åÖ·äì·àç                0         , ·ãï·âÉ          ...   \n",
       "1  [3                    0         , 1           ...   \n",
       "2  [·ã®·çí·ãõ                  0         , ·àò·åã·åà·à™·ã´       ...   \n",
       "3  [·ã®·àÖ·çÉ·äì·âµ                0         , ·ã®·àª·ãà·à≠        ...   \n",
       "4  [·ã®·àò·àò·åà·â¢·ã´               0         , ·à≥·àÖ·äï         ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "1  [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "2  [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "3  [B-PRODUCT, O, O, O, O, O, I-PRICE, I-PRICE, O...  \n",
       "4  [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, I...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Display the updated DataFrame\n",
    "df[['cleaned_text', 'tokens_labels', 'tokens', 'labels']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for training\n",
    "train_data = [(row['tokens'], row['labels']) for _, row in df.iterrows() if row['tokens'] and row['labels']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word in the original sentence\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yibabe\\Desktop\\10academyAIMweek-5\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # Tokenize the tokens column\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    # Create empty labels list\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get the word IDs after tokenization\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # If there is no word ID, append a label for special tokens (CLS, SEP, etc.)\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Label for the first sub-token of a word\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # Label for the subsequent sub-tokens of a word\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    # Add the labels to the tokenized inputs\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the tokenization and label alignment\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 180\n",
      "Validation dataset size: 20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assuming df1 is your DataFrame containing the tokenized data\n",
    "train_df, val_df = train_test_split(df1, test_size=0.1, random_state=42)  # 10% for validation\n",
    "\n",
    "# Convert DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Check the datasets\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labeled_message</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens_labels</th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·àù·äï·å£...</td>\n",
       "      <td>·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·àù·äï·å£...</td>\n",
       "      <td>([·ã®·àò·àò·åà·â¢·ã´               0         , ·àù·äï·å£·çç       ...</td>\n",
       "      <td>[·ã®·àò·àò·åà·â¢·ã´               0         , ·àù·äï·å£·çç        ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, I-PRICE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>·àà·ä•·àµ·àç·àù·äì               0          B-PRODUCT\\n·ä•·àù·äê...</td>\n",
       "      <td>·àà·ä•·àµ·àç·àù·äì               0          B-PRODUCT\\n·ä•·àù·äê...</td>\n",
       "      <td>([·àà·ä•·àµ·àç·àù·äì               0         , ·ä•·àù·äê·âµ       ...</td>\n",
       "      <td>[·àà·ä•·àµ·àç·àù·äì               0         , ·ä•·àù·äê·âµ        ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>350                  0          B-PRODUCT\\n·â•·à≠ ...</td>\n",
       "      <td>350                  0          B-PRODUCT\\n·â•·à≠ ...</td>\n",
       "      <td>([350                  0         , ·â•·à≠         ...</td>\n",
       "      <td>[350                  0         , ·â•·à≠          ...</td>\n",
       "      <td>[B-PRODUCT, I-PRICE, O, O, I-PRICE, I-PRICE, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>·ã®·â§·âµ                  0          B-PRODUCT\\n·àõ·àû·âÇ...</td>\n",
       "      <td>·ã®·â§·âµ                  0          B-PRODUCT\\n·àõ·àû·âÇ...</td>\n",
       "      <td>([·ã®·â§·âµ                  0         , ·àõ·àû·âÇ·ã´       ...</td>\n",
       "      <td>[·ã®·â§·âµ                  0         , ·àõ·àû·âÇ·ã´        ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6                    0          B-PRODUCT\\n·â∂·àé ...</td>\n",
       "      <td>6                    0          B-PRODUCT\\n·â∂·àé ...</td>\n",
       "      <td>([6                    0         , ·â∂·àé         ...</td>\n",
       "      <td>[6                    0         , ·â∂·àé          ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>·ã®·ä†·âµ·ä≠·àç·âµ·äì              0          B-PRODUCT\\n·çç·à´·çç...</td>\n",
       "      <td>·ã®·ä†·âµ·ä≠·àç·âµ·äì              0          B-PRODUCT\\n·çç·à´·çç...</td>\n",
       "      <td>([·ã®·ä†·âµ·ä≠·àç·âµ·äì              0         , ·çç·à´·çç·à¨       ...</td>\n",
       "      <td>[·ã®·ä†·âµ·ä≠·àç·âµ·äì              0         , ·çç·à´·çç·à¨        ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, I-PRICE, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>·ä©·àΩ·äì·ãé·äï                0          B-PRODUCT\\n·ã´·à≥·àù...</td>\n",
       "      <td>·ä©·àΩ·äì·ãé·äï                0          B-PRODUCT\\n·ã´·à≥·àù...</td>\n",
       "      <td>([·ä©·àΩ·äì·ãé·äï                0         , ·ã´·à≥·àù·à©       ...</td>\n",
       "      <td>[·ä©·àΩ·äì·ãé·äï                0         , ·ã´·à≥·àù·à©        ...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>·ã®·âÖ·àò·àõ                 0          B-PRODUCT\\n·âÖ·àò·àù...</td>\n",
       "      <td>·ã®·âÖ·àò·àõ                 0          B-PRODUCT\\n·âÖ·àò·àù...</td>\n",
       "      <td>([·ã®·âÖ·àò·àõ                 0         , ·âÖ·àò·àù·ã®·å®·ãç·ã®·àµ·ä≥·à≠·â°...</td>\n",
       "      <td>[·ã®·âÖ·àò·àõ                 0         , ·âÖ·àò·àù·ã®·å®·ãç·ã®·àµ·ä≥·à≠·â°·äì...</td>\n",
       "      <td>[B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·àò·çÄ·ã≥...</td>\n",
       "      <td>·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·àò·çÄ·ã≥...</td>\n",
       "      <td>([·ã®·àÖ·çÉ·äì·âµ                0         , ·àò·çÄ·ã≥·åÉ       ...</td>\n",
       "      <td>[·ã®·àÖ·çÉ·äì·âµ                0         , ·àò·çÄ·ã≥·åÉ        ...</td>\n",
       "      <td>[B-PRODUCT, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>·ä†·ãµ·à´·àª·âΩ·äï·ç¶              0          B-PRODUCT\\n·àò·åà·äì...</td>\n",
       "      <td>·ä†·ãµ·à´·àª·âΩ·äï·ç¶              0          B-PRODUCT\\n·àò·åà·äì...</td>\n",
       "      <td>([·ä†·ãµ·à´·àª·âΩ·äï·ç¶              0         , ·àò·åà·äì·äõ       ...</td>\n",
       "      <td>[·ä†·ãµ·à´·àª·âΩ·äï·ç¶              0         , ·àò·åà·äì·äõ        ...</td>\n",
       "      <td>[B-PRODUCT, I-LOC, I-LOC, I-LOC, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       labeled_message  \\\n",
       "124  ·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·àù·äï·å£...   \n",
       "16   ·àà·ä•·àµ·àç·àù·äì               0          B-PRODUCT\\n·ä•·àù·äê...   \n",
       "148  350                  0          B-PRODUCT\\n·â•·à≠ ...   \n",
       "93   ·ã®·â§·âµ                  0          B-PRODUCT\\n·àõ·àû·âÇ...   \n",
       "65   6                    0          B-PRODUCT\\n·â∂·àé ...   \n",
       "..                                                 ...   \n",
       "106  ·ã®·ä†·âµ·ä≠·àç·âµ·äì              0          B-PRODUCT\\n·çç·à´·çç...   \n",
       "14   ·ä©·àΩ·äì·ãé·äï                0          B-PRODUCT\\n·ã´·à≥·àù...   \n",
       "92   ·ã®·âÖ·àò·àõ                 0          B-PRODUCT\\n·âÖ·àò·àù...   \n",
       "179  ·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·àò·çÄ·ã≥...   \n",
       "102  ·ä†·ãµ·à´·àª·âΩ·äï·ç¶              0          B-PRODUCT\\n·àò·åà·äì...   \n",
       "\n",
       "                                          cleaned_text  \\\n",
       "124  ·ã®·àò·àò·åà·â¢·ã´               0          B-PRODUCT\\n·àù·äï·å£...   \n",
       "16   ·àà·ä•·àµ·àç·àù·äì               0          B-PRODUCT\\n·ä•·àù·äê...   \n",
       "148  350                  0          B-PRODUCT\\n·â•·à≠ ...   \n",
       "93   ·ã®·â§·âµ                  0          B-PRODUCT\\n·àõ·àû·âÇ...   \n",
       "65   6                    0          B-PRODUCT\\n·â∂·àé ...   \n",
       "..                                                 ...   \n",
       "106  ·ã®·ä†·âµ·ä≠·àç·âµ·äì              0          B-PRODUCT\\n·çç·à´·çç...   \n",
       "14   ·ä©·àΩ·äì·ãé·äï                0          B-PRODUCT\\n·ã´·à≥·àù...   \n",
       "92   ·ã®·âÖ·àò·àõ                 0          B-PRODUCT\\n·âÖ·àò·àù...   \n",
       "179  ·ã®·àÖ·çÉ·äì·âµ                0          B-PRODUCT\\n·àò·çÄ·ã≥...   \n",
       "102  ·ä†·ãµ·à´·àª·âΩ·äï·ç¶              0          B-PRODUCT\\n·àò·åà·äì...   \n",
       "\n",
       "                                         tokens_labels  \\\n",
       "124  ([·ã®·àò·àò·åà·â¢·ã´               0         , ·àù·äï·å£·çç       ...   \n",
       "16   ([·àà·ä•·àµ·àç·àù·äì               0         , ·ä•·àù·äê·âµ       ...   \n",
       "148  ([350                  0         , ·â•·à≠         ...   \n",
       "93   ([·ã®·â§·âµ                  0         , ·àõ·àû·âÇ·ã´       ...   \n",
       "65   ([6                    0         , ·â∂·àé         ...   \n",
       "..                                                 ...   \n",
       "106  ([·ã®·ä†·âµ·ä≠·àç·âµ·äì              0         , ·çç·à´·çç·à¨       ...   \n",
       "14   ([·ä©·àΩ·äì·ãé·äï                0         , ·ã´·à≥·àù·à©       ...   \n",
       "92   ([·ã®·âÖ·àò·àõ                 0         , ·âÖ·àò·àù·ã®·å®·ãç·ã®·àµ·ä≥·à≠·â°...   \n",
       "179  ([·ã®·àÖ·çÉ·äì·âµ                0         , ·àò·çÄ·ã≥·åÉ       ...   \n",
       "102  ([·ä†·ãµ·à´·àª·âΩ·äï·ç¶              0         , ·àò·åà·äì·äõ       ...   \n",
       "\n",
       "                                                tokens  \\\n",
       "124  [·ã®·àò·àò·åà·â¢·ã´               0         , ·àù·äï·å£·çç        ...   \n",
       "16   [·àà·ä•·àµ·àç·àù·äì               0         , ·ä•·àù·äê·âµ        ...   \n",
       "148  [350                  0         , ·â•·à≠          ...   \n",
       "93   [·ã®·â§·âµ                  0         , ·àõ·àû·âÇ·ã´        ...   \n",
       "65   [6                    0         , ·â∂·àé          ...   \n",
       "..                                                 ...   \n",
       "106  [·ã®·ä†·âµ·ä≠·àç·âµ·äì              0         , ·çç·à´·çç·à¨        ...   \n",
       "14   [·ä©·àΩ·äì·ãé·äï                0         , ·ã´·à≥·àù·à©        ...   \n",
       "92   [·ã®·âÖ·àò·àõ                 0         , ·âÖ·àò·àù·ã®·å®·ãç·ã®·àµ·ä≥·à≠·â°·äì...   \n",
       "179  [·ã®·àÖ·çÉ·äì·âµ                0         , ·àò·çÄ·ã≥·åÉ        ...   \n",
       "102  [·ä†·ãµ·à´·àª·âΩ·äï·ç¶              0         , ·àò·åà·äì·äõ        ...   \n",
       "\n",
       "                                                labels  \n",
       "124  [B-PRODUCT, O, O, O, O, O, O, O, O, O, I-PRICE...  \n",
       "16   [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "148  [B-PRODUCT, I-PRICE, O, O, I-PRICE, I-PRICE, O...  \n",
       "93   [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "65   [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "..                                                 ...  \n",
       "106  [B-PRODUCT, O, O, O, O, O, O, O, O, I-PRICE, I...  \n",
       "14   [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "92   [B-PRODUCT, O, O, O, O, O, O, O, O, O, O, O, O...  \n",
       "179                                  [B-PRODUCT, O, O]  \n",
       "102  [B-PRODUCT, I-LOC, I-LOC, I-LOC, O, O, O, O, O...  \n",
       "\n",
       "[180 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labeled_message', 'cleaned_text', 'tokens_labels', 'tokens', 'labels', '__index_level_0__'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # Tokenize the inputs\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], padding='max_length', truncation=True, is_split_into_words=True)\n",
    "\n",
    "    # Initialize labels\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['ner_tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(i)  # Get word ids corresponding to the tokens\n",
    "        label_ids = [-100] * len(tokenized_inputs['input_ids'])  # Default to -100 (ignore index)\n",
    "\n",
    "        for word_idx in set(word_ids):\n",
    "            if word_idx is None: \n",
    "                continue  # Skip special tokens\n",
    "            # Use the label for the first token of the word\n",
    "            label_ids[word_ids.index(word_idx)] = label[word_idx]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the tokenization and label alignment\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset.column_names)  # This should show 'input_ids', 'attention_mask', and 'labels'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yibabe\\Desktop\\10academyAIMweek-5\\env\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Step 7: Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory for model predictions and checkpoints\n",
    "    evaluation_strategy=\"epoch\",  # Evaluation strategy to adopt during training\n",
    "    save_strategy=\"epoch\",  # Save strategy to adopt during training\n",
    "    learning_rate=2e-5,  # Learning rate for optimization\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    num_train_epochs=3,  # Total number of training epochs\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    save_total_limit=2,  # Limit the total amount of checkpoints\n",
    "    load_best_model_at_end=True,  # Load the best model when finished training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 8: Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # The instantiated Transformers model to be trained\n",
    "    args=training_args,  # Training arguments, defined above\n",
    "    train_dataset=tokenized_dataset,  # Training dataset\n",
    "    eval_dataset=tokenized_dataset,  # Evaluation dataset\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2634 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 9: Fine-tune the model\n",
    "trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
